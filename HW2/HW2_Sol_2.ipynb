{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINM 250 Homework 2\n",
    "### TA Solutions\n",
    "\n",
    "**Note:** in this notebook, pay close attention to how virtually all of my code is functionalized. This is a good habit to get into, and will save you a lot of time on the midterm. By having a lot of different functions, I'm able to complete the entire assignment with (excluding multi-line statements for formatting purposes) 25 lines of code *total*. If I further exclude plotting/displaying code to make the answers look nice, the \"core\" code is only 12-15 lines long.\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting settings.\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# Pandas settings.\n",
    "pd.set_option(\"display.float_format\", lambda x: \"{:.4f}\".format(x))\n",
    "\n",
    "# Constants for risk metrics, return metrics, and annualization.\n",
    "RETURN_COLS = [\"Annualized Return\", \"Annualized Volatility\", \"Annualized Sharpe Ratio\"]\n",
    "RISK_COLS = [\n",
    "    \"Skewness\",\n",
    "    \"Excess Kurtosis\",\n",
    "    \"VaR (0.05)\",\n",
    "    \"CVaR (0.05)\",\n",
    "    \"Max Drawdown\",\n",
    "    \"Bottom\",\n",
    "    \"Peak\",\n",
    "    \"Recovery\",\n",
    "    \"Duration (days)\",\n",
    "]\n",
    "ADJ = 12\n",
    "\n",
    "\n",
    "def calc_return_metrics(data, as_df=False, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate return metrics for a given dataset. Specifically:\n",
    "    - Annualized Return\n",
    "    - Annualized Volatility\n",
    "    - Annualized Sharpe Ratio\n",
    "    - Annualized Sortino Ratio (not part of the course, but useful to know)\n",
    "\n",
    "    Args:\n",
    "        data : Returns time series.\n",
    "        as_df (bool, optional): Return a df or dict. Defaults to False.\n",
    "        adj (int, optional): Annualization. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame or dict: Summary of return metrics.\n",
    "    \"\"\"\n",
    "    summary = dict()\n",
    "    summary[\"Annualized Return\"] = data.mean() * adj\n",
    "    summary[\"Annualized Volatility\"] = data.std() * np.sqrt(adj)\n",
    "    summary[\"Annualized Sharpe Ratio\"] = (\n",
    "        summary[\"Annualized Return\"] / summary[\"Annualized Volatility\"]\n",
    "    )\n",
    "    summary[\"Annualized Sortino Ratio\"] = (\n",
    "        summary[\"Annualized Return\"] * np.sqrt(adj) / (data[data < 0].std())\n",
    "    )\n",
    "\n",
    "    # Here, we use what is known as a \"ternary operator\", usually denoted as \"condition ? if_true : if_false\",\n",
    "    # in other programming languages. This is equivalent to having an explicit if-else statement, but is more\n",
    "    # concise and can be written on a single line.\n",
    "    return pd.DataFrame(summary, index=data.columns) if as_df else summary\n",
    "\n",
    "\n",
    "def calc_risk_metrics(data, as_df=False, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate risk metrics for a given dataset. Specifically:\n",
    "    - Skewness\n",
    "    - Kurt\n",
    "    - VaR (0.05)\n",
    "    - CVaR (0.05)\n",
    "    - Max Return in a single period\n",
    "    - Min Return in a single period\n",
    "    - Peak\n",
    "    - Max Drawdown\n",
    "    - Bottom (of drawdown)\n",
    "    - Recovery (of drawdown)\n",
    "\n",
    "    Args:\n",
    "        data : Returns time series.\n",
    "        as_df (bool, optional): Return a df or a dictionary. Defaults to False.\n",
    "        adj (int, optional): Annualization. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame or Dictionary: Summary of risk metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    summary = dict()\n",
    "    summary[\"Skewness\"] = data.skew()\n",
    "    summary[\"Excess Kurtosis\"] = data.kurtosis()\n",
    "    summary[\"VaR (0.05)\"] = data.quantile(0.05, axis=0)\n",
    "    summary[\"CVaR (0.05)\"] = data[data <= data.quantile(0.05, axis=0)].mean()\n",
    "    summary[\"Min\"] = data.min()\n",
    "    summary[\"Max\"] = data.max()\n",
    "\n",
    "    # Cumulative returns on $1000\n",
    "    wealth_index = 1000 * (1 + data).cumprod()\n",
    "\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "\n",
    "    # Biggest difference between cumulative max and your current wealth\n",
    "    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
    "\n",
    "    summary[\"Max Drawdown\"] = drawdowns.min()\n",
    "\n",
    "    summary[\"Peak\"] = previous_peaks.idxmax()\n",
    "    summary[\"Bottom\"] = drawdowns.idxmin()\n",
    "\n",
    "    recovery_date = []\n",
    "    for col in wealth_index.columns:\n",
    "        prev_max = previous_peaks[col][: drawdowns[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin() :]]).T\n",
    "        recovery_date.append(\n",
    "            recovery_wealth[recovery_wealth[col] >= prev_max].index.min()\n",
    "        )\n",
    "\n",
    "    # Here, we use a list comprehension to check if the recovery date is null,\n",
    "    # meaning that the drawdown has not yet recovered.\n",
    "    summary[\"Recovery\"] = [\"-\" if pd.isnull(i) else i for i in recovery_date]\n",
    "\n",
    "    # Note that here we use the zip() function to iterator over two lists at the same time.\n",
    "    # This is equivalent to using a for loop over the indices of the lists, and then getting\n",
    "    # the values at those indices.\n",
    "    summary[\"Duration (days)\"] = [\n",
    "        (i - j).days if i != \"-\" else \"-\"\n",
    "        for i, j in zip(summary[\"Recovery\"], summary[\"Bottom\"])\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(summary, index=data.columns) if as_df else summary\n",
    "\n",
    "\n",
    "def calc_perf_metrics(data, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data : Returns time series.\n",
    "        adj (int, optional): Annualization. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Summary of performance metrics.\n",
    "    \"\"\"\n",
    "    # This is a more advanced Python concept, known as \"dictionary unpacking\".\n",
    "    # It is equivalent to joining the two dictionaries together using a for loop\n",
    "    # over the keys, but is much more concise.\n",
    "\n",
    "    # There is a similar concept for lists, known as \"list unpacking\" or \"iterator unpacking\",\n",
    "    # as it works for any iterable object. We use this via tha *list operator. So, if we\n",
    "    # wanted to join two lists together, we would do: list3 = [*list1, *list2].\n",
    "\n",
    "    summary = {**calc_return_metrics(data, adj), **calc_risk_metrics(data, adj)}\n",
    "    summary[\"Calmar Ratio\"] = summary[\"Annualized Return\"] / abs(\n",
    "        summary[\"Max Drawdown\"]\n",
    "    )\n",
    "    return pd.DataFrame(summary, index=data.columns)\n",
    "\n",
    "\n",
    "def calc_univ_regr(y, X, intercept=True, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate a univariate regression of y on X. Note that both X and y\n",
    "    need to be one-dimensional.\n",
    "\n",
    "    Args:\n",
    "        y : target variable\n",
    "        X : independent variable\n",
    "        intercept (bool, optional): Fit the regression with an intercept or not. Defaults to True.\n",
    "        adj (int, optional): What to adjust the returns by. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Summary of regression results\n",
    "    \"\"\"\n",
    "    X_down = X[y < 0]\n",
    "    y_down = y[y < 0]\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    "        X_down = sm.add_constant(X_down)\n",
    "\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    results = model.fit()\n",
    "\n",
    "    # More ternary operators.\n",
    "    inter = results.params[0] if inter else 0\n",
    "    beta = results.params[1] if inter else results.params[0]\n",
    "\n",
    "    summary = dict()\n",
    "    summary[\"Alpha\"] = inter * adj\n",
    "    summary[\"Beta\"] = beta\n",
    "    # Note: Downside Beta is not part of the course, but is a useful metric to know.\n",
    "    down_mod = sm.OLS(y_down, X_down, missing=\"drop\").fit()\n",
    "    summary[\"Downside Beta\"] = down_mod.params[1] if inter else down_mod.params[0]\n",
    "    summary[\"R-Squared\"] = results.rsquared\n",
    "    summary[\"Treynor Ratio\"] = (y.mean() / beta) * adj\n",
    "    summary[\"Information Ratio\"] = (inter / results.resid.std()) * np.sqrt(adj)\n",
    "    summary[\"Tracking Error\"] = inter / summary[\"Information Ratio\"] if intercept else results.resid.std() * np.sqrt(adj)\n",
    "\n",
    "    return pd.DataFrame(summary, index=[y.name])\n",
    "\n",
    "\n",
    "def calc_multi_regr(y, X, intercept=True, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate a multivariate regression of y on X. Adds useful metrics such\n",
    "    as the Information Ratio and Tracking Error. Note that we can't calculate\n",
    "    Treynor Ratio or Downside Beta here.\n",
    "\n",
    "    Args:\n",
    "        y : target variable\n",
    "        X : independent variables\n",
    "        intercept (bool, optional): Defaults to True.\n",
    "        adj (int, optional): Annualization factor. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Summary of regression results\n",
    "    \"\"\"\n",
    "    if intercept:\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    results = model.fit()\n",
    "    summary = dict()\n",
    "\n",
    "    inter = results.params[0] if intercept else 0\n",
    "    betas = results.params[1:] if intercept else results.params\n",
    "\n",
    "    summary[\"Alpha\"] = inter * adj\n",
    "    summary[\"R-Squared\"] = results.rsquared\n",
    "\n",
    "    X_cols = X.columns[1:] if intercept else X.columns\n",
    "\n",
    "    for i, col in enumerate(X_cols):\n",
    "        summary[f\"{col} Beta\"] = betas[i]\n",
    "\n",
    "    summary[\"Information Ratio\"] = (inter / results.resid.std()) * np.sqrt(adj)\n",
    "    summary[\"Tracking Error\"] = (\n",
    "        inter / summary[\"Information Ratio\"]\n",
    "        if intercept\n",
    "        else results.resid.std() * np.sqrt(adj)\n",
    "    )\n",
    "    return pd.DataFrame(summary, index=[y.name])\n",
    "\n",
    "\n",
    "def calc_iter_regr(y, X, intercept=True, one_to_many=False, adj=12):\n",
    "    \"\"\"\n",
    "    Iterative regression for checking one X column against many different y columns,\n",
    "    or vice versa. \"one_to_many=True\" means that we are checking one X column against many\n",
    "    y columns, and \"one_to_many=False\" means that we are checking many X columns against a\n",
    "    single y column.\n",
    "\n",
    "    Args:\n",
    "        y : Target variable(s)\n",
    "        X : Independent variable(s)\n",
    "        intercept (bool, optional): Defaults to True.\n",
    "        one_to_many (bool, optional): Which way to run the regression. Defaults to False.\n",
    "        adj (int, optional): Annualization. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame : Summary of regression results.\n",
    "    \"\"\"\n",
    "    if one_to_many:\n",
    "        summary = pd.concat(\n",
    "            [calc_univ_regr(y[col], X, intercept, adj) for col in y.columns], axis=0\n",
    "        )\n",
    "        summary.index = y.columns\n",
    "        return summary\n",
    "    else:\n",
    "        summary = pd.concat(\n",
    "            [calc_univ_regr(y, X[col], intercept, adj) for col in X.columns], axis=0\n",
    "        )\n",
    "        summary.index = X.columns\n",
    "        return summary\n",
    "\n",
    "\n",
    "def plot_corr_matrix(corrs):\n",
    "    # Correlation helper function.\n",
    "    return sns.heatmap(\n",
    "        corrs,\n",
    "        annot=True,\n",
    "        cmap=\"coolwarm\",\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=0.7,\n",
    "        annot_kws={\"size\": 10},\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        cbar_kws={\"shrink\": 0.75},\n",
    "    )\n",
    "\n",
    "\n",
    "def print_max_min_corr(corrs):\n",
    "    # Correlation helper function.\n",
    "    corr_series = corrs.unstack()\n",
    "    corr_series = corr_series[corr_series != 1]\n",
    "\n",
    "    max_corr = corr_series.abs().agg([\"idxmax\", \"max\"]).T\n",
    "    min_corr = corr_series.abs().agg([\"idxmin\", \"min\"]).T\n",
    "    min_corr_raw = corr_series.agg([\"idxmin\", \"min\"]).T\n",
    "    max_corr, max_corr_val = max_corr[\"idxmax\"], max_corr[\"max\"]\n",
    "    min_corr, min_corr_val = min_corr[\"idxmin\"], min_corr[\"min\"]\n",
    "    min_corr_raw, min_corr_raw_val = min_corr_raw[\"idxmin\"], min_corr_raw[\"min\"]\n",
    "\n",
    "    print(\n",
    "        f\"Max Corr (by absolute value): {max_corr[0]} and {max_corr[1]} with a correlation of {max_corr_val:.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Min Corr (by absolute value): {min_corr[0]} and {min_corr[1]} with a correlation of {min_corr_val:.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Min Corr (raw): {min_corr_raw[0]} and {min_corr_raw[1]} with a correlation of {min_corr_raw_val:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Those who read through the \"How to Avoid Typing the Same Variables Over and Over Again\" section\n",
    "# of TA Review number 2 (https://github.com/MarkHendricks/finm-quant-2023/blob/main/reviews/TA_Review_2.ipynb)\n",
    "# which I skipped to save for later, may notice that this is a great place for a partial function.\n",
    "\n",
    "# Load data.\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "get_data = partial(\n",
    "    pd.read_excel, \"../data/proshares_analysis_data.xlsx\", index_col=0, parse_dates=[0]\n",
    ")\n",
    "\n",
    "rets_hf = get_data(sheet_name=\"hedge_fund_series\")\n",
    "rets_ml = get_data(sheet_name=\"merrill_factors\")\n",
    "rets_other = get_data(sheet_name=\"other_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T13:30:19.906631500Z",
     "start_time": "2023-07-10T13:30:17.315275Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T13:30:19.984720900Z",
     "start_time": "2023-07-10T13:30:19.910629800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                Annualized Return  Annualized Volatility  \\\nHFRIFWI Index              0.0414                 0.0604   \nMLEIFCTR Index             0.0307                 0.0571   \nMLEIFCTX Index             0.0292                 0.0570   \nHDG US Equity              0.0194                 0.0596   \nQAI US Equity              0.0170                 0.0499   \n\n                Annualized Sharpe Ratio  \nHFRIFWI Index                    0.6860  \nMLEIFCTR Index                   0.5367  \nMLEIFCTX Index                   0.5131  \nHDG US Equity                    0.3263  \nQAI US Equity                    0.3399  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Annualized Return</th>\n      <th>Annualized Volatility</th>\n      <th>Annualized Sharpe Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>HFRIFWI Index</th>\n      <td>0.0414</td>\n      <td>0.0604</td>\n      <td>0.6860</td>\n    </tr>\n    <tr>\n      <th>MLEIFCTR Index</th>\n      <td>0.0307</td>\n      <td>0.0571</td>\n      <td>0.5367</td>\n    </tr>\n    <tr>\n      <th>MLEIFCTX Index</th>\n      <td>0.0292</td>\n      <td>0.0570</td>\n      <td>0.5131</td>\n    </tr>\n    <tr>\n      <th>HDG US Equity</th>\n      <td>0.0194</td>\n      <td>0.0596</td>\n      <td>0.3263</td>\n    </tr>\n    <tr>\n      <th>QAI US Equity</th>\n      <td>0.0170</td>\n      <td>0.0499</td>\n      <td>0.3399</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = calc_perf_metrics(rets_hf)\n",
    "metrics[RETURN_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T13:30:20.096356200Z",
     "start_time": "2023-07-10T13:30:19.986054800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                Skewness  Excess Kurtosis  VaR (0.05)  CVaR (0.05)  \\\nHFRIFWI Index    -0.9791           5.9551     -0.0253      -0.0375   \nMLEIFCTR Index   -0.2450           1.6896     -0.0288      -0.0359   \nMLEIFCTX Index   -0.2310           1.6579     -0.0291      -0.0358   \nHDG US Equity    -0.2339           1.7991     -0.0312      -0.0376   \nQAI US Equity    -0.4613           1.8260     -0.0206      -0.0327   \n\n                Max Drawdown     Bottom       Peak             Recovery  \\\nHFRIFWI Index        -0.1155 2020-03-31 2021-10-31  2020-08-31 00:00:00   \nMLEIFCTR Index       -0.1243 2022-09-30 2021-06-30                    -   \nMLEIFCTX Index       -0.1244 2022-09-30 2021-06-30                    -   \nHDG US Equity        -0.1407 2022-09-30 2021-06-30                    -   \nQAI US Equity        -0.1377 2022-09-30 2021-06-30                    -   \n\n               Duration (days)  \nHFRIFWI Index              153  \nMLEIFCTR Index               -  \nMLEIFCTX Index               -  \nHDG US Equity                -  \nQAI US Equity                -  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Skewness</th>\n      <th>Excess Kurtosis</th>\n      <th>VaR (0.05)</th>\n      <th>CVaR (0.05)</th>\n      <th>Max Drawdown</th>\n      <th>Bottom</th>\n      <th>Peak</th>\n      <th>Recovery</th>\n      <th>Duration (days)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>HFRIFWI Index</th>\n      <td>-0.9791</td>\n      <td>5.9551</td>\n      <td>-0.0253</td>\n      <td>-0.0375</td>\n      <td>-0.1155</td>\n      <td>2020-03-31</td>\n      <td>2021-10-31</td>\n      <td>2020-08-31 00:00:00</td>\n      <td>153</td>\n    </tr>\n    <tr>\n      <th>MLEIFCTR Index</th>\n      <td>-0.2450</td>\n      <td>1.6896</td>\n      <td>-0.0288</td>\n      <td>-0.0359</td>\n      <td>-0.1243</td>\n      <td>2022-09-30</td>\n      <td>2021-06-30</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>MLEIFCTX Index</th>\n      <td>-0.2310</td>\n      <td>1.6579</td>\n      <td>-0.0291</td>\n      <td>-0.0358</td>\n      <td>-0.1244</td>\n      <td>2022-09-30</td>\n      <td>2021-06-30</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>HDG US Equity</th>\n      <td>-0.2339</td>\n      <td>1.7991</td>\n      <td>-0.0312</td>\n      <td>-0.0376</td>\n      <td>-0.1407</td>\n      <td>2022-09-30</td>\n      <td>2021-06-30</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>QAI US Equity</th>\n      <td>-0.4613</td>\n      <td>1.8260</td>\n      <td>-0.0206</td>\n      <td>-0.0327</td>\n      <td>-0.1377</td>\n      <td>2022-09-30</td>\n      <td>2021-06-30</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[RISK_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that all except HFRIFWI have not yet recovered from their maximum drawdowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T13:30:20.915341200Z",
     "start_time": "2023-07-10T13:30:20.023852500Z"
    }
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'inter' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25240\\180678139.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mspy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrets_ml\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"SPY US Equity\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mregression_metrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_iter_regr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrets_hf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mone_to_many\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mregression_metrics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25240\\832466642.py\u001B[0m in \u001B[0;36mcalc_iter_regr\u001B[1;34m(y, X, intercept, one_to_many, adj)\u001B[0m\n\u001B[0;32m    256\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mone_to_many\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m         summary = pd.concat(\n\u001B[1;32m--> 258\u001B[1;33m             \u001B[1;33m[\u001B[0m\u001B[0mcalc_univ_regr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mintercept\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[1;32min\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    259\u001B[0m         )\n\u001B[0;32m    260\u001B[0m         \u001B[0msummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25240\\832466642.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    256\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mone_to_many\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m         summary = pd.concat(\n\u001B[1;32m--> 258\u001B[1;33m             \u001B[1;33m[\u001B[0m\u001B[0mcalc_univ_regr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mintercept\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[1;32min\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    259\u001B[0m         )\n\u001B[0;32m    260\u001B[0m         \u001B[0msummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25240\\832466642.py\u001B[0m in \u001B[0;36mcalc_univ_regr\u001B[1;34m(y, X, intercept, adj)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    179\u001B[0m     \u001B[1;31m# More ternary operators.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 180\u001B[1;33m     \u001B[0minter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minter\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    181\u001B[0m     \u001B[0mbeta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minter\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'inter' referenced before assignment"
     ]
    }
   ],
   "source": [
    "spy = rets_ml[\"SPY US Equity\"]\n",
    "regression_metrics = calc_iter_regr(rets_hf, spy, one_to_many=True)\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "### 4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T13:30:20.909340800Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_spy = pd.concat([metrics, calc_perf_metrics(spy.to_frame(\"SPY\"))], axis=0)\n",
    "metrics_spy[RETURN_COLS + RISK_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that it depends on the investors risk preference. Namely, SPY has better risk adjusted returns (higher Sharpe) than all members of the hedge fund series. It also has better kurtosis, with an excess of kurtosis of 0.8 compared to between 1.6 and 6 for the hedge fund series. It's mean is also much larger, and is between 3 and 10 times larger than the hedge fund series.\n",
    "\n",
    "However, when looking at other metrics, such as VaR, the hedge fund series has both a lower VaR and CVaR than SPY. We can further confirm this tail-risk that SPY has by looking at the Max Drawdown, which is 24% for SPY, compared with 11-15% for the hedge fund series. So, it would seem that SPY performs better when looking at mean and variance, but worse when looking at measures of tail risk.\n",
    "\n",
    "Another point is that the hedge fund series is highly correlated with SPY, with correlations between 0.7 and 0.8. This indicates that most of the variance of the hedge funds can be explained by SPY, and so the hedge funds are perhaps not providing much diversification. All of the hedge funds have negative alpha (meaning that they fail to beat the market). Though, their betas are not *too* extreme, and their downside betas are actually lower than their betas, indicating that they are not as exposed to downside risk as SPY -- so they are providing some \"hedge\" against downside risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T13:30:20.911340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get HDG US Equity and QAI US Equity, and concat both their regression_metrics and metrics\n",
    "pd.concat(\n",
    "    [\n",
    "        metrics.T[[\"HDG US Equity\", \"QAI US Equity\"]],\n",
    "        regression_metrics.T[[\"HDG US Equity\", \"QAI US Equity\"]],\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAI performs marginally better when looking at market correlation, but they are basically the same. Both have very similar Sharpe ratios, although QAI has lower vol, and HDG has a higher mean. Both have negative skew and an excess kurtosis of around 1.8 (although QAI has higher skew). Both have similar VaR and CVaR, although QAI performs better on both (slightly).\n",
    "\n",
    "However, where I think the argument is stronger for QAI is when looking at the correlation with SPY. It has a correlation of 0.74, compared to HDG's 0.78. Moreover, the market beta for QAI is smaller, at around 0.29, compared with HDG's 0.36. It also has a slightly larger alpha, meaning that less of it's returns can be explained by the market. It has a lower downside beta, indicating it is less exposed to downside risk than HDG.\n",
    "\n",
    "Though I would argue they are very similar, and unsurprisingly are 88% correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_hf[[\"HDG US Equity\", \"QAI US Equity\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c\n",
    "\n",
    "I would say yes; all have very similar risk metrics, returns, Sharpe ratio's, etc. HFRI has slightly better returns and a higher Sharpe. The biggest difference is in skew/kurt: HFRI has much higher kurtosis than both the ML series and HDG, with an excess kurtosis of around 6, compared to 1.7-1.8 for the others. It also has much bigger skew, at around -0.97, compared with -0.23-0.24 for the others. \n",
    "\n",
    "So, I would say that HFRI has bigger tails and more skew than the others, but HDG and the ML series capture most of it's properties. From Question 5, we see that the ML series is 90% correlated with HFRI, and HDG is 88% correlated with HFRI.\n",
    "\n",
    "We can further confirm this via regression.\n",
    "\n",
    "**Note** due to extremely high multicollinearity we probably do not want to run a regression with our $X$ inputs being these 3 factors, so we will run 3 separate regressions, one for each factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    calc_univ_regr(\n",
    "        rets_hf[\"HFRIFWI Index\"], rets_hf[\"HDG US Equity\"], intercept=True\n",
    "    ).rename({\"Beta\": \"HDG Beta\"}, axis=1)\n",
    ")\n",
    "\n",
    "display(\n",
    "    calc_univ_regr(\n",
    "        rets_hf[\"HFRIFWI Index\"], rets_hf[\"MLEIFCTR Index\"], intercept=True\n",
    "    ).rename({\"Beta\": \"ML Factor 1 Beta\"}, axis=1)\n",
    ")\n",
    "\n",
    "display(\n",
    "    calc_univ_regr(\n",
    "        rets_hf[\"HFRIFWI Index\"], rets_hf[\"MLEIFCTX Index\"], intercept=True\n",
    "    ).rename({\"Beta\": \"ML Factor 2 Beta\"}, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared values are all very high, betas are close to 1. Thus, it does seem that the ML series and HDG capture most of the properties of HFRI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "### 5.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = pd.concat([rets_hf, rets_ml], axis=1).corr()\n",
    "\n",
    "fig, _ = plt.subplots(figsize=(8, 8))\n",
    "ax = plot_corr_matrix(corr_matrix)\n",
    "\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_min_corr(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = pd.concat(\n",
    "    [\n",
    "        calc_multi_regr(rets_hf[\"HFRIFWI Index\"], rets_ml).T,\n",
    "        calc_multi_regr(rets_hf[\"HFRIFWI Index\"], rets_ml, intercept=False).T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "replication.columns = [\"HFRI (Intercept)\", \"HFRI (No Intercept)\"]\n",
    "replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to replicate 84% of the variance of the returns of HFRI using the Merrill Lynch factors. We have an intercept of 0.0178 (annualized), or 0.0015 non-annualized.\n",
    "\n",
    "We also have quite reasonable betas, with the largest being 15% of our portfolio, and the smallest being -37% of our portfolio. This indicates that we are not overly exposed to one factor. However, the -37% in 3-month T-bills is a little bit concerning, as the interpretation of this beta is that we are shorting a lot of T-bills (borrowing money) to invest in the other factors. But, -37% is not too much leverage (and is well within the range of a hedge fund - and would be considered very low leverage for a T-bill position). So, I would say that this is not too concerning.\n",
    "\n",
    "Overall, the replication works quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use a new Python concept, called a defaultdict. This is a dictionary that\n",
    "# automatically creates a new key if it doesn't exist. We can pass it a type in the constructor,\n",
    "# and it will create a new key with that type if it doesn't exist and then we can interact with\n",
    "# it as if it were a normal dictionary. So, since we have a list, we can do summary[key].append(value),\n",
    "# without having to first do summary[key] = [].\n",
    "from collections import defaultdict\n",
    "\n",
    "summary = defaultdict(list)\n",
    "\n",
    "for idx in range(60, len(rets_hf), 1):\n",
    "    # Get the data for ML factors and HFRI\n",
    "    X = rets_ml.iloc[idx - 60 : idx].copy()\n",
    "    y = rets_hf.iloc[idx - 60 : idx, 0].copy()\n",
    "\n",
    "    oos_y = rets_hf.iloc[idx, 0]\n",
    "    oos_X = rets_ml.iloc[idx, :].copy()\n",
    "\n",
    "    # Calculate the regression coefficients\n",
    "    regr = sm.OLS(y, sm.add_constant(X), missing=\"drop\").fit()\n",
    "\n",
    "    for jdx, coeff in enumerate(regr.params.index):\n",
    "        summary[coeff].append(regr.params[jdx])\n",
    "\n",
    "    # Get predicted y by multiplying the coefficients by the X values and\n",
    "    # adding the intercept\n",
    "    y_pred = regr.params[0] + (regr.params[1:] @ oos_X)\n",
    "\n",
    "    summary[\"Replicated\"].append(y_pred)\n",
    "    summary[\"Actual\"].append(oos_y)\n",
    "\n",
    "\n",
    "summary = pd.DataFrame(summary, index=rets_hf.index[60:])\n",
    "\n",
    "# Calculate OOS R-Squared\n",
    "oos_rsquared = (\n",
    "    1 - (summary[\"Actual\"] - summary[\"Replicated\"]).var() / summary[\"Actual\"].var()\n",
    ")\n",
    "print(f\"OOS-R-Squared: {oos_rsquared:.2%}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(summary.index, summary[\"Actual\"], label=\"Actual\", color=\"blue\")\n",
    "ax.plot(summary.index, summary[\"Replicated\"], label=\"Replicated\", color=\"red\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Make ticklabels every 6 months and label them as %yyy-%mm\n",
    "ax.set_xticks(summary.index[::6])\n",
    "ax.set_xticklabels(\n",
    "    [f\"{i.year}-{i.month:02d}\" for i in summary.index[::6]],\n",
    "    rotation=45,\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "\n",
    "ax.set_title(\"HFRI Replication\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does an extremely good job of replicating, with an out-of-sample R-squared of 80%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "I already did this in 6, but:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does better than with an intercept. Additionally, all of the betas are positive, and of much more reasonable sizes. Although, the tracking error is about 10x bigger, and of course we no longer have alpha or an information ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use non-negative least squares to get the weights\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "hfri = rets_hf[\"HFRIFWI Index\"]\n",
    "\n",
    "non_neg = LinearRegression(fit_intercept=True, positive=True).fit(rets_ml, hfri)\n",
    "\n",
    "non_neg_r2 = non_neg.score(rets_ml, hfri)\n",
    "print(f\"NNLS R-Squared: {non_neg_r2:.2%}\")\n",
    "\n",
    "# Display the weights\n",
    "weights = pd.DataFrame(\n",
    "    [non_neg.intercept_, *non_neg.coef_],\n",
    "    index=[\"const\", *rets_ml.columns],\n",
    "    columns=[\"Weights\"],\n",
    ")\n",
    "display(weights)\n",
    "\n",
    "# Plot the replication\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(hfri.index, hfri, label=\"Actual\", color=\"blue\")\n",
    "ax.plot(hfri.index, non_neg.predict(rets_ml), label=\"Replicated\", color=\"red\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_title(\"HFRI Replication\")\n",
    "ax.set_xticks(hfri.index[::8])\n",
    "ax.set_xticklabels(\n",
    "    [f\"{i.year}-{i.month:02d}\" for i in hfri.index[::8]],\n",
    "    rotation=45,\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM to get the weights, suppose we cap the USGG3M at 0.1 and SPY at 0.05\n",
    "glm_weights = sm.GLM(\n",
    "    hfri, sm.add_constant(rets_ml), family=sm.families.Gaussian()\n",
    ").fit_constrained([\"USGG3M Index = 0.1\", \"SPY US Equity = 0.05\"])\n",
    "\n",
    "# Calculate R-2\n",
    "\n",
    "# This is from: https://stackoverflow.com/questions/26541899/why-doesnt-statsmodels-glm-have-r2-in-results\n",
    "sst_val = sum(map(lambda x: np.power(x, 2), y - np.mean(y)))\n",
    "sse_val = sum(map(lambda x: np.power(x, 2), glm_weights.resid_response))\n",
    "r2 = 1.0 - sse_val / sst_val\n",
    "\n",
    "print(f\"GLM R-Squared: {r2:.2%}\")\n",
    "\n",
    "# Display the weights\n",
    "display(pd.DataFrame(glm_weights.params, columns=[\"Weights\"]))\n",
    "\n",
    "# Plot the replication\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(hfri.index, hfri, label=\"Actual\", color=\"blue\")\n",
    "ax.plot(\n",
    "    hfri.index,\n",
    "    glm_weights.predict(sm.add_constant(rets_ml)),\n",
    "    label=\"Replicated\",\n",
    "    color=\"red\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xticks(hfri.index[::8])\n",
    "ax.set_xticklabels(\n",
    "    [f\"{i.year}-{i.month:02d}\" for i in hfri.index[::8]],\n",
    "    rotation=45,\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "ax.set_title(\"HFRI Replication\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_multi_regr(rets_other[\"HEFA US Equity\"], rets_ml, intercept=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it does look like a currency-hedged version of EFA. In particular, we have a beta of 0.95 on EFA, meaning that it tracks it quite well. Of course, it's hard to say how well the hedge performs without other data, but at least judging by this regression HEFA and EFA are closely related.\n",
    "\n",
    "## 4.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_multi_regr(rets_other[\"TRVCI Index\"], rets_ml, intercept=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High betas on SPY and USGG3M. However, a somewhat low R-squared of 0.7. I would argue (perhaps cynically) that this does describe venture capital somewhat well; lots of leverage (high beta on USGG3M) but returns that are no different than the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress rets_other['TAIL US Equity'] on SPY. However, we make an adjustment: we add a \"put like\" factor to SPY.\n",
    "\n",
    "rets_spy = rets_ml[[\"SPY US Equity\"]].copy()\n",
    "rets_spy[\"Put Factor\"] = np.maximum(0, rets_spy[\"SPY US Equity\"])\n",
    "\n",
    "# Run the regression without the put factor\n",
    "display(calc_multi_regr(rets_other[\"TAIL US Equity\"], spy, intercept=True).T)\n",
    "\n",
    "# And with\n",
    "display(calc_multi_regr(rets_other[\"TAIL US Equity\"], rets_spy, intercept=True).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to say. The correlation to SPY is quite low, but it does have a non-trivial beta on the \"put like\" factor. But overall I would say no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.a\n",
    "\n",
    "Because they have to continually re-lever in order to track the index at 3x. This means that they have a decay over time and the expected return of holding these ETF to infinity is -100%. As an example:\n",
    "\n",
    "- Day 1: SPY is at 100, and closes at 110 (10% up). The 3x ETF is at 100, and closes at 130 (30% up).\n",
    "- Day 2: SPY is at 110, and closes at 100 (-9.1% down). The 3x ETF is at 130, and closes at 94.51 (-27.3% down).\n",
    "\n",
    "So, even though SPY is back to where it started, the 3x ETF is down 5.49%. This is because it has to re-lever every day, and so it is buying high and selling low. For example, during the financial crisis when there were many large movements down, we would expect these ETFs to lose a lot of value due to continually having to re-lever. This is also why they have high management feed (UPRO has 0.91% management fee, compared to 0.09% for SPY, or 0.03% for VOO).\n",
    "\n",
    "Pro Shares also wins the dubious award of most value lost by an ETF creator: https://twitter.com/quantian1/status/1600168896459251712?s=20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if we maintained perfect 3x leverage, we would expect UPRO vs. SPY to have a beta of 3 on SPY.\n",
    "# In fact, Pro-Shares even claims this on their website:\n",
    "# 3x Short: https://www.proshares.com/globalassets/proshares/fact-sheet/prosharesfactsheetspxu.pdf\n",
    "# 3x Long: https://www.proshares.com/globalassets/proshares/fact-sheet/prosharesfactsheetupro.pdf\n",
    "upro_regr = calc_univ_regr(rets_other[\"UPRO US Equity\"], spy, intercept=True).T\n",
    "upro_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a remarkably good job of staying 3x levered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but for SPXU -- -3x leverage.\n",
    "spxu_regr = calc_univ_regr(rets_other[\"SPXU US Equity\"], spy, intercept=True).T\n",
    "spxu_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a worse job. Perhaps this is unsurprising given the SPY has trended up, meaning that being 3x short it means that you are constantly losing money and needing to re-lever (as described above). What is particularly interesting is the Downside Beta, which is -2 compared to -2.6 for normal beta. So this indicates that the upside perhaps is asymmetric. Importantly, though, this is an unfair comparison: we are looking at monthly returns on an ETF which explicitly says it targets ***daily*** -3x returns, so we should expect some divergence when looking at monthly returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "spy_df = {\n",
    "    \"SPY\": (1 + rets_spy[\"SPY US Equity\"]).cumprod() - 1,\n",
    "    \"UPRO\": (1 + rets_other[\"UPRO US Equity\"]).cumprod() - 1,\n",
    "    \"SPXU\": (1 + rets_other[\"SPXU US Equity\"]).cumprod() - 1,\n",
    "}\n",
    "\n",
    "spy_df = pd.DataFrame(spy_df, index=rets_spy[\"SPY US Equity\"].index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(spy_df.index, spy_df[\"SPY\"], label=\"SPY\", color=\"blue\")\n",
    "ax.plot(spy_df.index, spy_df[\"UPRO\"], label=\"UPRO\", color=\"green\")\n",
    "ax.plot(spy_df.index, spy_df[\"SPXU\"], label=\"SPXU\", color=\"red\")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax.axhline(-1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xticks(spy_df.index[::10])\n",
    "ax.set_xticklabels(\n",
    "    [f\"{i.year}-{i.month:02d}\" for i in spy_df.index[::10]],\n",
    "    rotation=45,\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "ax.set_title(\"SPY vs. UPRO vs. SPXU\")\n",
    "ax.set_yticks([-1] + [i for i in range(0, 27, 2)])\n",
    "ax.set_ylabel(\"Cumulative Return\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-3x SPY decays to -100% returns pretty quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
